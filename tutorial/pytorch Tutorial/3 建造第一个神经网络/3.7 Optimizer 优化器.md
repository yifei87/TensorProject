学习资料:

[本节的全部代码](https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/306_optimizer.py)

[Tensorflow 的 Optimizer 代码](https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/304_optimizer.py)

[我制作的 训练优化器 动画简介](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-06-speed-up-learning/)

[PyTorch 优化器网页](http://pytorch.org/docs/master/optim.html)

[PyTorch 官网](http://pytorch.org/)


----------
[toc]

# 伪数据
为了对比各种优化器的效果, 我们需要有一些数据, 今天我们还是自己编一些伪数据, 这批数据是这样的:
![这里写图片描述](https://morvanzhou.github.io/static/results/torch/3-6-1.png)

```python
import torch
import torch.utils.data as Data
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

torch.manual_seed(1)    # reproducible

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12

# fake dataset
x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)
y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))

# plot dataset
plt.scatter(x.numpy(), y.numpy())
plt.show()

# 使用上节内容提到的 data loader
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)
loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)
```

# 每个优化器优化一个神经网络
为了对比每一种优化器, 我们给他们各自创建一个神经网络, 但这个神经网络都来自同一个 Net 形式.

```python
# 默认的 network 形式
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(1, 20)   # hidden layer
        self.predict = torch.nn.Linear(20, 1)   # output layer

    def forward(self, x):
        x = F.relu(self.hidden(x))      # activation function for hidden layer
        x = self.predict(x)             # linear output
        return x

# 为每个优化器创建一个 net
net_SGD         = Net()
net_Momentum    = Net()
net_RMSprop     = Net()
net_Adam        = Net()
nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]
```

# 优化器 Optimizer
接下来在创建不同的优化器, 用来训练不同的网络. 并创建一个 loss_func 用来计算误差. 我们用几种常见的优化器, SGD, Momentum, RMSprop, Adam.

```python
# different optimizers
opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)
opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

loss_func = torch.nn.MSELoss()
losses_his = [[], [], [], []]   # 记录 training 时不同神经网络的 loss
```

# 训练/出图
接下来训练和 loss 画图.

```python
for epoch in range(EPOCH):
    print('Epoch: ', epoch)
    for step, (batch_x, batch_y) in enumerate(loader):
        b_x = Variable(batch_x)  # 务必要用 Variable 包一下
        b_y = Variable(batch_y)

        # 对每个优化器, 优化属于他的神经网络
        for net, opt, l_his in zip(nets, optimizers, losses_his):
            output = net(b_x)              # get output for every net
            loss = loss_func(output, b_y)  # compute loss for every net
            opt.zero_grad()                # clear gradients for next train
            loss.backward()                # backpropagation, compute gradients
            opt.step()                     # apply gradients
            l_his.append(loss.data[0])     # loss recoder
```

![这里写图片描述](https://morvanzhou.github.io/static/results/torch/3-6-2.png)

SGD 是最普通的优化器, 也可以说没有加速效果, 而 Momentum 是 SGD 的改良版, 它加入了动量原则. 后面的 RMSprop 又是 Momentum 的升级版. 而 Adam 又是 RMSprop 的升级版. 不过从这个结果中我们看到, Adam 的效果似乎比 RMSprop 要差一点. 所以说并不是越先进的优化器, 结果越佳. 我们在自己的试验中可以尝试不同的优化器, 找到那个最适合你数据/网络的优化器.

# pytorch 代码

```python
import torch
import torch.utils.data as Data
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

torch.manual_seed(1)    # reproducible

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12

# fake dataset
x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)
y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))

# plot dataset
plt.scatter(x.numpy(), y.numpy())
plt.show()

# put dateset into torch dataset
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)
loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)


# default network
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(1, 20)   # hidden layer
        self.predict = torch.nn.Linear(20, 1)   # output layer

    def forward(self, x):
        x = F.relu(self.hidden(x))      # activation function for hidden layer
        x = self.predict(x)             # linear output
        return x

# different nets
net_SGD         = Net()
net_Momentum    = Net()
net_RMSprop     = Net()
net_Adam        = Net()
nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]

# different optimizers
opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)
opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

loss_func = torch.nn.MSELoss()
losses_his = [[], [], [], []]   # record loss

# training
for epoch in range(EPOCH):
    print('Epoch: ', epoch)
    for step, (batch_x, batch_y) in enumerate(loader):          # for each training step
        b_x = Variable(batch_x)
        b_y = Variable(batch_y)

        for net, opt, l_his in zip(nets, optimizers, losses_his):
            output = net(b_x)              # get output for every net
            loss = loss_func(output, b_y)  # compute loss for every net
            opt.zero_grad()                # clear gradients for next train
            loss.backward()                # backpropagation, compute gradients
            opt.step()                     # apply gradients
            l_his.append(loss.data[0])     # loss recoder

labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']
for i, l_his in enumerate(losses_his):
    plt.plot(l_his, label=labels[i])
plt.legend(loc='best')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.ylim((0, 0.2))
plt.show()
```

# 对应的tensorflow代码

```python
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np

np.random.seed(1)    # reproducible

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12

# fake dataset
x=np.linspace(-1,1,1000)[:,np.newaxis]
y=x**2+0.1*np.random.rand(x.shape[0],x.shape[1])

# plot dataset
plt.scatter(x, y)
plt.show()

data=np.append(x,y,axis=1) # 1000x2
def shutffle_batch(data,batch_size):
    min_after_dequeue = 10
    capacity = min_after_dequeue + 3 * batch_size
    batch_data = tf.train.shuffle_batch(
            [tf.train.input_producer(data, shuffle=True).dequeue()],
            batch_size=batch_size,
            num_threads=4,
            capacity=capacity,
            min_after_dequeue=min_after_dequeue)
    return batch_data


_x=tf.placeholder(tf.float32,[None,1])
_y=tf.placeholder(tf.float32,[None,1])

# default network
class Net(object):
    def __init__(self,_x,_y):
        self._x=_x
        self._y=_y
    def Linear(self):
        hidden=tf.layers.dense(tf.convert_to_tensor(self._x),20,tf.nn.relu)
        predict=tf.layers.dense(hidden,1)
        return predict
    def Loss(self):
        return tf.reduce_mean(tf.square(self.Linear()-self._y))

# different nets
net_SGD_loss         = Net(_x,_y).Loss()
net_Momentum_loss    = Net(_x,_y).Loss()
net_RMSprop_loss     = Net(_x,_y).Loss()
net_Adam_loss        = Net(_x,_y).Loss()
nets_loss = [net_SGD_loss, net_Momentum_loss, net_RMSprop_loss, net_Adam_loss]

# different optimizers
opt_SGD         = tf.train.GradientDescentOptimizer(LR).minimize(net_SGD_loss)
opt_Momentum    = tf.train.MomentumOptimizer(LR,0.8).minimize(net_Momentum_loss)
opt_RMSprop     = tf.train.RMSPropOptimizer(LR,0.9).minimize(net_RMSprop_loss)
opt_Adam        = tf.train.AdamOptimizer(LR,0.9,0.999).minimize(net_Adam_loss)
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]


batch_data =shutffle_batch(data,BATCH_SIZE)

sess=tf.InteractiveSession()
tf.global_variables_initializer().run()
tf.local_variables_initializer().run()


losses_his = [[], [], [], []]   # record loss

coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)

try:
    while not coord.should_stop():
        # training
        for epoch in range(EPOCH):
            print('Epoch: ', epoch)
            for step in range(int(np.ceil(len(data) / BATCH_SIZE))):
                _data = batch_data.eval()
                # sess.run(optimizers,feed_dict={_x:_data[:,0:-1],_y:_data[:,-1]})
                for loss1, opt, l_his in zip(nets_loss, optimizers, losses_his):
                    _,loss=sess.run([opt,loss1], feed_dict={_x: _data[:, 0:-1], _y: _data[:, -1][:,np.newaxis]})
                    l_his.append(loss)

        labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']
        for i, l_his in enumerate(losses_his):
            plt.plot(l_his, label=labels[i])
        plt.legend(loc='best')
        plt.xlabel('Steps')
        plt.ylabel('Loss')
        plt.ylim((0, 0.2))
        plt.show()

        # else:
        break
except tf.errors.OutOfRangeError:
    print('Done training -- epoch limit reached')

finally:
    coord.request_stop()
    sess.close()
coord.join(threads)
```
