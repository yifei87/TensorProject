学习资料:

- [本节的全部代码](https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py)
- [Tensorflow 的 50行 RNN 代码](https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/402_RNN_classification.py)
- [我制作的 循环神经网络 RNN 动画简介](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-3-RNN/)
- [我制作的 循环神经网络 LSTM 动画简介](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-4-LSTM/)
- [PyTorch 官网](http://pytorch.org/)


----------
[toc]

# 训练数据
我们要用到的数据就是这样的一些数据, 我们想要用 sin 的曲线预测出 cos 的曲线.
![这里写图片描述](https://morvanzhou.github.io/static/results/torch/4-3-2.png)

```python
import torch
from torch import nn
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

torch.manual_seed(1)    # reproducible

# Hyper Parameters
TIME_STEP = 10      # rnn time step / image height
INPUT_SIZE = 1      # rnn input size / image width
LR = 0.02           # learning rate
DOWNLOAD_MNIST = False  # set to True if haven't download the data
```

# RNN模型
这一次的 RNN, 我们对每一个 r_out 都得放到 Linear 中去计算出预测的 output, 所以我们能用一个 for loop 来循环计算. 这点是 **Tensorflow 望尘莫及的**! 除了这点, 还有一些动态的过程都可以在[这个教程](https://morvanzhou.github.io/tutorials/machine-learning/torch/5-01-dynamic/)中查看, 看看我们的 PyTorch 和 Tensorflow 到底哪家强.

```python
class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()

        self.rnn = nn.RNN(  # 这回一个普通的 RNN 就能胜任
            input_size=1,
            hidden_size=32,     # rnn hidden unit
            num_layers=1,       # 有几层 RNN layers
            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)
        )
        self.out = nn.Linear(32, 1)

    def forward(self, x, h_state):  # 因为 hidden state 是连续的, 所以我们要一直传递这一个 state
        # x (batch, time_step, input_size)
        # h_state (n_layers, batch, hidden_size)
        # r_out (batch, time_step, output_size)
        r_out, h_state = self.rnn(x, h_state)   # h_state 也要作为 RNN 的一个输入

        outs = []    # 保存所有时间点的预测值
        for time_step in range(r_out.size(1)):    # 对每一个时间点计算 output
            outs.append(self.out(r_out[:, time_step, :]))
        return torch.stack(outs, dim=1), h_state


rnn = RNN()
print(rnn)
"""
RNN (
  (rnn): RNN(1, 32, batch_first=True)
  (out): Linear (32 -> 1)
)
"""
```
其实熟悉 RNN 的朋友应该知道, forward 过程中的对每个时间点求输出还有一招使得计算量比较小的. 不过上面的内容主要是为了呈现 PyTorch 在动态构图上的优势, 所以我用了一个 for loop 来搭建那套输出系统. 下面介绍一个替换方式. 使用 reshape 的方式整批计算.

```python
def forward(self, x, h_state):
    r_out, h_state = self.rnn(x, h_state)
    r_out_reshaped = r_out.view(-1, HIDDEN_SIZE) # to 2D data
    outs = self.linear_layer(r_out_reshaped)
    outs = outs.view(-1, TIME_STEP, INPUT_SIZE)  # to 3D data
```
# 训练
下面的代码就能实现动图的效果啦~开心, 可以看出, 我们使用 x 作为输入的 sin 值, 然后 y 作为想要拟合的输出, cos 值. 因为他们两条曲线是存在某种关系的, 所以我们就能用 sin 来预测 cos. rnn 会理解他们的关系, 并用里面的参数分析出来这个时刻 sin 曲线上的点如何对应上 cos 曲线上的点.

![这里写图片描述](https://morvanzhou.github.io/static/results/torch/4-3-1.gif)

```python
optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all rnn parameters
loss_func = nn.MSELoss()

h_state = None   # 要使用初始 hidden state, 可以设成 None

for step in range(60):
    start, end = step * np.pi, (step+1)*np.pi   # time steps
    # sin 预测 cos
    steps = np.linspace(start, end, 10, dtype=np.float32)
    x_np = np.sin(steps)    # float32 for converting torch FloatTensor
    y_np = np.cos(steps)

    x = Variable(torch.from_numpy(x_np[np.newaxis, :, np.newaxis]))    # shape (batch, time_step, input_size)
    y = Variable(torch.from_numpy(y_np[np.newaxis, :, np.newaxis]))

    prediction, h_state = rnn(x, h_state)   # rnn 对于每个 step 的 prediction, 还有最后一个 step 的 h_state
    # !!  下一步十分重要 !!
    h_state = Variable(h_state.data)  # 要把 h_state 重新包装一下才能放入下一个 iteration, 不然会报错

    loss = loss_func(prediction, y)     # cross entropy loss
    optimizer.zero_grad()               # clear gradients for this training step
    loss.backward()                     # backpropagation, compute gradients
    optimizer.step()                    # apply gradients
```
# pytorch代码

```Python
import torch
from torch import nn
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

torch.manual_seed(1)    # reproducible

# Hyper Parameters
TIME_STEP = 10      # rnn time step 序列个数
INPUT_SIZE = 1      # rnn input size 每个序列的长度
LR = 0.02           # learning rate

# show data
steps = np.linspace(0, np.pi*2, 100, dtype=np.float32)
x_np = np.sin(steps)  # shape (100,)    # float32 for converting torch FloatTensor
y_np = np.cos(steps)  # shape (100,)
plt.plot(steps, y_np, 'r-', label='target (cos)')
plt.plot(steps, x_np, 'b-', label='input (sin)')
plt.legend(loc='best')
plt.show()


class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()

        self.rnn = nn.RNN(
            input_size=INPUT_SIZE,
            hidden_size=32,     # rnn hidden unit 隐藏节点数
            num_layers=1,       # number of rnn layer
            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)
        )
        self.out = nn.Linear(32, 1)

    def forward(self, x, h_state):
        # x (batch, time_step, input_size)
        # h_state (n_layers, batch, hidden_size)
        # r_out (batch, time_step, hidden_size)
        r_out, h_state = self.rnn(x, h_state)

        outs = []    # save all predictions
        for time_step in range(r_out.size(1)):    # calculate output for each time step
            outs.append(self.out(r_out[:, time_step, :])) # 将每一个状态输出存放到outs
        return torch.stack(outs, dim=1), h_state  # dim=1 按行合并成tensor

        # instead, for simplicity, you can replace above codes by follows
        # r_out = r_out.view(-1, 32)
        # outs = self.out(r_out)
        # return outs, h_state

rnn = RNN()
print(rnn)

optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters
loss_func = nn.MSELoss()

h_state = None      # for initial hidden state

plt.figure(1, figsize=(12, 5))
plt.ion()           # continuously plot

for step in range(60):
    start, end = step * np.pi, (step+1)*np.pi   # time range
    # use sin predicts cos
    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)
    x_np = np.sin(steps)  # (10,)   # float32 for converting torch FloatTensor
    y_np = np.cos(steps)  # (10,)

    x = Variable(torch.from_numpy(x_np[np.newaxis, :, np.newaxis]))    # shape (batch, time_step, input_size)
    y = Variable(torch.from_numpy(y_np[np.newaxis, :, np.newaxis]))  # shape (1,10,1)

    prediction, h_state = rnn(x, h_state)   # rnn output
    # !! next step is important !!
    h_state = Variable(h_state.data)        # repack the hidden state, break the connection from last iteration

    loss = loss_func(prediction, y)         # cross entropy loss
    optimizer.zero_grad()                   # clear gradients for this training step
    loss.backward()                         # backpropagation, compute gradients
    optimizer.step()                        # apply gradients

    # plotting
    plt.plot(steps, y_np.flatten(), 'r-')
    plt.plot(steps, prediction.data.numpy().flatten(), 'b-')
    plt.draw(); plt.pause(0.05)

plt.ioff()
plt.show()
```

# tensorflow代码
详细参考[深度学习（08）_RNN-LSTM循环神经网络-03-Tensorflow进阶实现](http://blog.csdn.net/u013082989/article/details/73693392)

[github](https://github.com/lawlite19/Blog-Back-Up/blob/master/code/rnn/rnn_tensorflow/rnn_general_implement.py)

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# from tensorflow.contrib import rnn

np.random.seed(1)  # reproducible

# Hyper Parameters
TIME_STEP = 10  # rnn time step 序列个数
INPUT_SIZE = 1  # rnn input size 每个序列的长度
LR = 0.02  # learning rate
batch_size = 1
hidden_size = 32

# show data
steps = np.linspace(0, np.pi * 2, 100, dtype=np.float32)
x_np = np.sin(steps)  # shape (100,)    # float32 for converting torch FloatTensor
y_np = np.cos(steps)  # shape (100,)
plt.plot(steps, y_np, 'r-', label='target (cos)')
plt.plot(steps, x_np, 'b-', label='input (sin)')
plt.legend(loc='best')
plt.show()


def RNN(x):
    # x (batch, time_step, input_size)
    # h_state (n_layers, batch, hidden_size)
    # r_out (batch, time_step, hidden_size)
    rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)
    init_state = rnn_cell.zero_state(batch_size, tf.float32)

    r_out, h_state = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=x,
                                       initial_state=init_state)

    rnn_outputs = tf.reshape(r_out, [-1, hidden_size])  # 转成二维的矩阵

    outs = tf.layers.dense(rnn_outputs, 1)
    return outs, h_state, init_state


_x = tf.placeholder(tf.float32, [None, 1], name='x')
_y = tf.placeholder(tf.float32, [None, 1], name='y')

logits, h_state, init_state = RNN(tf.reshape(_x, [1, 10, 1]))
total_loss = tf.losses.mean_squared_error(labels=_y,predictions=logits)
train_step = tf.train.AdamOptimizer(LR).minimize(total_loss)

plt.figure(1, figsize=(12, 5))
plt.ion()  # continuously plot

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    training_losses = []
    training_loss = 0
    training_state = None

    for step in range(60):
        start, end = step * np.pi, (step + 1) * np.pi  # time range
        # use sin predicts cos
        steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)
        x_np = np.sin(steps)  # (10,)   # float32 for converting torch FloatTensor
        y_np = np.cos(steps)  # (10,)

        x = x_np[:, np.newaxis]  # shape (time_step, input_size)
        y = y_np[:, np.newaxis]  # shape (10,1)

        feed_dict = {_x: x, _y: y}
        if training_state is not None:
            feed_dict[init_state] = training_state

        _, training_loss_, training_state = sess.run([train_step, total_loss, h_state], feed_dict)
        training_loss += training_loss_
        print('step: {0}的平均损失值：{1}'.format(step, training_loss / (step + 1)))
        training_losses.append(training_loss / steps)

        # plotting
        plt.plot(steps, y_np.flatten(), 'r-')
        plt.plot(steps, logits.eval(feed_dict).flatten(), 'b-')
        plt.draw();
        plt.pause(0.05)

    plt.ioff()
    plt.show()
```

## 2

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


# Hyper Parameters
TIME_STEP = 10       # rnn time step
INPUT_SIZE = 1      # rnn input size
CELL_SIZE = 32      # rnn cell size
LR = 0.02           # learning rate

# show data
steps = np.linspace(0, np.pi*2, 100, dtype=np.float32)
x_np = np.sin(steps); y_np = np.cos(steps)    # float32 for converting torch FloatTensor
plt.plot(steps, y_np, 'r-', label='target (cos)'); plt.plot(steps, x_np, 'b-', label='input (sin)')
plt.legend(loc='best'); plt.show()

# tensorflow placeholders
tf_x = tf.placeholder(tf.float32, [None, TIME_STEP, INPUT_SIZE])        # shape(batch, 5, 1)
tf_y = tf.placeholder(tf.float32, [None, TIME_STEP, INPUT_SIZE])          # input y

# RNN
rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=CELL_SIZE)
init_s = rnn_cell.zero_state(batch_size=1, dtype=tf.float32)    # very first hidden state
outputs, final_s = tf.nn.dynamic_rnn(
    rnn_cell,                   # cell you have chosen
    tf_x,                       # input
    initial_state=init_s,       # the initial hidden state
    time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)
)
outs2D = tf.reshape(outputs, [-1, CELL_SIZE])                       # reshape 3D output to 2D for fully connected layer
net_outs2D = tf.layers.dense(outs2D, INPUT_SIZE)
outs = tf.reshape(net_outs2D, [-1, TIME_STEP, INPUT_SIZE])          # reshape back to 3D

loss = tf.losses.mean_squared_error(labels=tf_y, predictions=outs)  # compute cost
train_op = tf.train.AdamOptimizer(LR).minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())     # initialize var in graph

plt.figure(1, figsize=(12, 5)); plt.ion()       # continuously plot

for step in range(60):
    start, end = step * np.pi, (step+1)*np.pi   # time range
    # use sin predicts cos
    steps = np.linspace(start, end, TIME_STEP)
    x = np.sin(steps)[np.newaxis, :, np.newaxis]    # shape (batch, time_step, input_size)
    y = np.cos(steps)[np.newaxis, :, np.newaxis]
    if 'final_s_' not in globals():                 # first state, no any hidden state
        feed_dict = {tf_x: x, tf_y: y}
    else:                                           # has hidden state, so pass it to rnn
        feed_dict = {tf_x: x, tf_y: y, init_s: final_s_}
    _, pred_, final_s_ = sess.run([train_op, outs, final_s], feed_dict)     # train

    # plotting
    plt.plot(steps, y.flatten(), 'r-'); plt.plot(steps, pred_.flatten(), 'b-')
    plt.ylim((-1.2, 1.2)); plt.draw(); plt.pause(0.05)

plt.ioff(); plt.show()
```








